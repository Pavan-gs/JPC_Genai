{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948ef0a-487a-4113-ab34-9941de7bce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Installs and Imports (Run once – ~1-2 min first time)\n",
    "!pip install transformers torch nltk rouge-score boto3 -q  # Transformers for GPT-2; NLTK/Rouge for metrics; Boto3 for future Bedrock\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Load GPT-2 (local, free model – 1.5B params, runs on CPU/GPU)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix for padding in generation\n",
    "\n",
    "# ROUGE Scorer instance\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "print(\"Setup done! GPT-2 ready for prompting demos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3fa13-7158-4ebe-afd8-e48d76884560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Shot Demo: Classify API error log (No examples – direct instruction)\n",
    "# Ref: Ideal output for scoring (like a 'gold standard' unit test)\n",
    "ref_output = \"Critical. 500 error indicates server-side failure, risking downtime.\"\n",
    "\n",
    "# Prompt: System-like instruction + user query\n",
    "system_prompt = \"You are an API log classifier for full-stack monitoring. Output: Label (Critical/Warning/Info) + 1-sentence reason.\"\n",
    "user_prompt = \"Log: 'HTTP 500: Database connection failed during user query.'\"\n",
    "full_prompt = f\"{system_prompt}\\n{user_prompt}\\n\"  # Concat for GPT-2 input\n",
    "\n",
    "# Invoke GPT-2: Generate response (tune params as needed)\n",
    "print(\"GPT-2 Zero-Shot Output:\")\n",
    "inputs = tokenizer.encode(full_prompt, return_tensors='pt')  # Tokenize prompt\n",
    "outputs = model.generate(inputs, max_length=100, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)  # Generate\n",
    "gen_output = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(full_prompt, '').strip()  # Extract clean response\n",
    "print(gen_output)\n",
    "\n",
    "# Scoring Function: BLEU/ROUGE (Run this after generation)\n",
    "def explain_and_score(gen, ref):\n",
    "    gen_tokens = nltk.word_tokenize(gen.lower())  # Tokenize for metrics\n",
    "    ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "    \n",
    "    # BLEU Explained: Like a 'precision checker' for translations – measures how many word sequences (n-grams: 1-4 words) in your generated output match the reference exactly.\n",
    "    # How it Scores: Geometric average of n-gram precisions (e.g., unigram match % * bigram %...) + penalty if output too short. Range: 0-1 (1=identical).\n",
    "    # When to Use: For exact-match tasks like classifications or code gen (e.g., dev bug labels) – important to detect if model hallucinates wrong terms.\n",
    "    # Importance: Quantifies 'accuracy' without manual review; e.g., in prod, threshold BLEU >0.6 for auto-approval. Reliable ~80% vs. humans, but penalizes synonyms (e.g., 'failure' vs. 'crash' lowers score).\n",
    "    bleu_score = sentence_bleu([ref_tokens], gen_tokens)\n",
    "    print(f\"\\nBLEU Score: {bleu_score:.3f} (Higher = better exact phrase match; use for precise dev tasks like error labels.)\")\n",
    "    \n",
    "    # ROUGE Explained: Like a 'recall checker' for summaries – measures how much content from the ref is captured in your output (ignores extra fluff).\n",
    "    # How it Scores: F1 (precision + recall balance) for overlaps. ROUGE-1: Single words; ROUGE-L: Longest common sequences. Range: 0-1.\n",
    "    # When to Use: For explanatory outputs like reasons or summaries (e.g., data eng log reports) – important to ensure key details aren't missed.\n",
    "    # Importance: Helps scale eval in pipelines; e.g., if ROUGE-L <0.5, prompt needs more guidance. Reliable for completeness, but favors longer outputs; combine with BLEU for balance.\n",
    "    rouge_scores = scorer.score(ref, gen)\n",
    "    print(f\"ROUGE-1 F1: {rouge_scores['rouge1'].fmeasure:.3f} (Word-level coverage)\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.3f} (Sequence completeness)\")\n",
    "\n",
    "explain_and_score(gen_output, ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f484c5-56da-4da7-b1b8-b4329e7b2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Shot Demo: Extract API status code (1 example for guidance)\n",
    "ref_output = \"500\"\n",
    "\n",
    "system_prompt = \"You are an API response parser for backend devs. Output only the status code number.\"\n",
    "user_prompt = \"Example: 'Response: 404 Not Found' → 404\\nNow: 'Response: 500 Internal Server Error'\"\n",
    "full_prompt = f\"{system_prompt}\\n{user_prompt}\\n\"\n",
    "\n",
    "# Invoke GPT-2\n",
    "print(\"GPT-2 One-Shot Output:\")\n",
    "inputs = tokenizer.encode(full_prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=50, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "gen_output = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(full_prompt, '').strip()\n",
    "print(gen_output)\n",
    "\n",
    "explain_and_score(gen_output, ref_output)  # Reuse function from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d12a3-31f5-41ac-9695-51780a68989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Demo: Extract KV pairs from ETL log (2-3 examples for patterns)\n",
    "ref_output = \"{'job_id': 'etl-123', 'duration_sec': 45, 'rows_processed': 10000}\"\n",
    "\n",
    "system_prompt = \"You are a log KV extractor for data pipelines. Output as dict: {'key': value}\"\n",
    "user_prompt = \"\"\"Ex1: '{\"task\": \"load\", \"time\": 10}' → {'task': 'load', 'time': 10}\n",
    "Ex2: '{\"error\": \"overflow\", \"code\": 123}' → {'error': 'overflow', 'code': 123}\n",
    "Ex3: '{\"batch\": \"daily\", \"size\": 5000}' → {'batch': 'daily', 'size': 5000}\n",
    "Now: '{\"job_id\": \"etl-123\", \"duration_sec\": 45, \"rows_processed\": 10000}'\"\"\"\n",
    "full_prompt = f\"{system_prompt}\\n{user_prompt}\\n\"\n",
    "\n",
    "# Invoke GPT-2\n",
    "print(\"GPT-2 Few-Shot Output:\")\n",
    "inputs = tokenizer.encode(full_prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=100, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "gen_output = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(full_prompt, '').strip()\n",
    "print(gen_output)\n",
    "\n",
    "explain_and_score(gen_output, ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bea1ed-76fb-4274-95bf-38c660ff6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT Demo: Optimize slow SQL query (Step-by-step reasoning)\n",
    "ref_output = \"Step 1: Full scan slow – add index on user_id. Step 2: Limit columns. Fixed: SELECT name, email FROM users WHERE user_id > 1000 LIMIT 50;\"\n",
    "\n",
    "system_prompt = \"You are a SQL optimizer for data engineers. Think step-by-step, then output fixed query.\"\n",
    "user_prompt = \"Query: SELECT * FROM users WHERE user_id > 1000; Issue: Slow on large table.\"\n",
    "full_prompt = f\"{system_prompt}\\n{user_prompt}\\n\"\n",
    "\n",
    "# Invoke GPT-2 (higher max_length for chain)\n",
    "print(\"GPT-2 CoT Output:\")\n",
    "inputs = tokenizer.encode(full_prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=150, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "gen_output = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(full_prompt, '').strip()\n",
    "print(gen_output)\n",
    "\n",
    "explain_and_score(gen_output, ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e8fe1-c0f9-49c9-a7d2-ccbab40764fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Consistency Demo: Estimate API latency (3 paths + consensus)\n",
    "ref_output = \"Consensus: 150 ms (base 100 ms + 50 ms overhead).\"\n",
    "\n",
    "system_prompt = \"You are a latency estimator for microservices. Generate 3 step-by-step paths, then consensus in ms.\"\n",
    "user_prompt = \"Scenario: Base GET request 100 ms, with DB query adding 20-80 ms overhead.\"\n",
    "full_prompt = f\"{system_prompt}\\n{user_prompt}\\n\"\n",
    "\n",
    "# Invoke GPT-2 3x (vary temp for 'multiple paths')\n",
    "print(\"GPT-2 Self-Consistency Outputs:\")\n",
    "paths = []\n",
    "for temp in [0.1, 0.3, 0.5]:  # Lower temp = consistent; higher = varied\n",
    "    inputs = tokenizer.encode(full_prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=150, temperature=temp, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    path = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(full_prompt, '').strip()\n",
    "    paths.append(path)\n",
    "    print(f\"Path (Temp {temp}): {path}\")\n",
    "\n",
    "gen_output = ' '.join(paths)  # Concat paths for scoring (or manual pick consensus)\n",
    "explain_and_score(gen_output, ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb4f6ce-074f-4588-b4e3-aa6acee1996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049752b-42a7-48b7-b7f6-1c009ae91e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
